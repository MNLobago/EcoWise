{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "JbLd6pUB_6Qw"
   },
   "source": [
    "<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n",
    "\n",
    "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
    "\n",
    "\n",
    "# Introduction\n",
    "\n",
    "This notebook will demonstrate three things:\n",
    "\n",
    "1. How to fine-tune Gemma model using LoRA\n",
    "2. Creation of a specialised class to query about Kaggle features\n",
    "3. Some results of querying about Kaggle Docs\n",
    "\n",
    "This work is largely based on previous work. Here I list the sources:\n",
    "\n",
    "1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n",
    "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
    "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)\n",
    "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
    "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
    "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
    "7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n",
    "8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n",
    "9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n",
    "10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
    "\n",
    "\n",
    "**Let's go**!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVbdQU09_6Q1"
   },
   "source": [
    "# What is Gemma 2?\n",
    "\n",
    "Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all.\n",
    "\n",
    "Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components:\n",
    "* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n",
    "* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n",
    "* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n",
    "\n",
    "To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WbkaQaV_6Q2"
   },
   "source": [
    "# What is LoRA?  \n",
    "\n",
    "**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hb1Yfw5B_6Q2"
   },
   "source": [
    "# How we proceed?\n",
    "\n",
    "For fine-tunning with LoRA, we will follow the steps:\n",
    "\n",
    "1. Install prerequisites\n",
    "2. Load and process the data for fine-tuning\n",
    "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
    "4. Perform fine-tuning\n",
    "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqykmF-v_6Q3"
   },
   "source": [
    "# Prerequisites\n",
    "\n",
    "\n",
    "## Install packages\n",
    "\n",
    "We start by installing `keras-nlp` and `keras` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9we3AQKiKpB0",
    "outputId": "5fdd0c4e-078b-4fa2-b946-3c842690e38c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  1 20:36:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   32C    P0              49W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfXg_CrnDOUK"
   },
   "outputs": [],
   "source": [
    "#/content/drive/MyDrive/Hungers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fXSB-y51BI4c",
    "outputId": "9b1c969f-c73e-4457-c4b8-f8933fd6ca9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SM81nR-lDRg1"
   },
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle  # Create the .kaggle directory if it doesn't exist\n",
    "!cp /content/drive/MyDrive/Hungers_data/kaggle.json ~/.kaggle/  # Copy the file\n",
    "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions for the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dm9z1de6GH0Q"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load kaggle.json from Google Drive\n",
    "with open('/content/drive/MyDrive/Hungers_data/kaggle.json') as f:\n",
    "    userdata = json.load(f)  # Load the JSON data\n",
    "\n",
    "# Set environment variables\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('username')  # Extract username\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('key')            # Extract key\n",
    "\n",
    "# Verify that the environment variables are set\n",
    "#print(\"KAGGLE_USERNAME:\", os.environ[\"KAGGLE_USERNAME\"])\n",
    "#print(\"KAGGLE_KEY:\", os.environ[\"KAGGLE_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w7HWtpjeDlgX",
    "outputId": "6d88a4da-fa7a-4fe1-9076-c7511e48bada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: kaggle models [-h] {instances,get,list,init,create,delete,update} ...\n",
      "kaggle models: error: argument command: invalid choice: 'download' (choose from 'instances', 'get', 'list', 'init', 'create', 'delete', 'update')\n"
     ]
    }
   ],
   "source": [
    "!kaggle models download -m keras/gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:13:38.123623Z",
     "iopub.status.busy": "2024-10-27T14:13:38.123157Z",
     "iopub.status.idle": "2024-10-27T14:14:21.265319Z",
     "shell.execute_reply": "2024-10-27T14:14:21.264120Z",
     "shell.execute_reply.started": "2024-10-27T14:13:38.123595Z"
    },
    "id": "P-klIBWD_6Q3",
    "outputId": "853a4b9f-993f-43fa-ad7a-8fdde70d6978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/644.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m634.9/644.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.1/644.1 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tf-keras 2.17.0 requires tensorflow<2.18,>=2.17, but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
    "!pip install -q -U keras-nlp\n",
    "!pip install -q -U keras>=3\n",
    "!pip install -q -U kagglehub --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcMr8E19_6Q7"
   },
   "source": [
    "## Import packages\n",
    "\n",
    "Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n",
    "\n",
    "Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:43.408556Z",
     "iopub.status.busy": "2024-10-27T14:17:43.407386Z",
     "iopub.status.idle": "2024-10-27T14:17:59.147958Z",
     "shell.execute_reply": "2024-10-27T14:17:59.146923Z",
     "shell.execute_reply.started": "2024-10-27T14:17:43.408514Z"
    },
    "id": "-5Kkx8Cp_6Q8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
    "import keras\n",
    "import keras_nlp\n",
    "import kagglehub\n",
    "\n",
    "\n",
    "#from kaggle_secrets import UserSecretsClient\n",
    "#user_secrets = UserSecretsClient()\n",
    "#os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n",
    "#os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas() # progress bar for pandas\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVcSHLL6_6Q8"
   },
   "source": [
    "## Configurations\n",
    "\n",
    "\n",
    "We use a `Config` class to group the information needed to control the fine-tuning process:\n",
    "* random seed\n",
    "* dataset path\n",
    "* preset - name of pretrained Gemma 2\n",
    "* sequence length - this is the maximum size of input sequence for training\n",
    "* batch size - size of the input batch in training, x 2 as two GPUs\n",
    "* lora rank - rank for LoRA, higher means more trainable parameters\n",
    "* learning rate used in the train\n",
    "* epochs - number of epochs for train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:59.150496Z",
     "iopub.status.busy": "2024-10-27T14:17:59.149796Z",
     "iopub.status.idle": "2024-10-27T14:17:59.156214Z",
     "shell.execute_reply": "2024-10-27T14:17:59.155284Z",
     "shell.execute_reply.started": "2024-10-27T14:17:59.150451Z"
    },
    "id": "iPh1I2gK_6Q8"
   },
   "outputs": [],
   "source": [
    "class Config:\n",
    "    seed = 42\n",
    "    dataset_path = \"/content/drive/MyDrive/Hungers_data\"\n",
    "    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n",
    "    sequence_length = 512 # max size of input sequence for training\n",
    "    batch_size = 4 # size of the input batch in training\n",
    "    lora_rank = 6 # rank for LoRA, higher means more trainable parameters\n",
    "    learning_rate=5e-5 # learning rate used in train\n",
    "    epochs = 5 # number of epochs to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7H-R8kq_6Q9"
   },
   "source": [
    "Set a random seed for results reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:17:59.157630Z",
     "iopub.status.busy": "2024-10-27T14:17:59.157314Z",
     "iopub.status.idle": "2024-10-27T14:17:59.167091Z",
     "shell.execute_reply": "2024-10-27T14:17:59.166295Z",
     "shell.execute_reply.started": "2024-10-27T14:17:59.157605Z"
    },
    "id": "xNmFKjfC_6Q9"
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(Config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KishRbOv_6Q9"
   },
   "source": [
    "# Load the data\n",
    "\n",
    "\n",
    "We load the data we will use for fine-tunining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:05.780320Z",
     "iopub.status.busy": "2024-10-27T14:18:05.779917Z",
     "iopub.status.idle": "2024-10-27T14:18:05.966861Z",
     "shell.execute_reply": "2024-10-27T14:18:05.966083Z",
     "shell.execute_reply.started": "2024-10-27T14:18:05.780291Z"
    },
    "id": "6woXxHqi_6Q9"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = []\n",
    "with open(f\"{Config.dataset_path}/high_volume_carbon_footprint_qa.json\") as file:\n",
    "    content = file.read()\n",
    "    try:\n",
    "        features_list = json.loads(content)\n",
    "        for features in features_list:\n",
    "            # Check if 'question' and 'answer' keys exist\n",
    "            if 'question' in features and 'answer' in features:\n",
    "                # Update the template to use the correct keys\n",
    "                template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
    "                data.append(template.format(**features))\n",
    "            else:\n",
    "                print(f\"Missing keys in features: {features}\")\n",
    "        #data = data[:2000]  # Limit to 1000 examples\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON: {e}\")\n",
    "\n",
    "# Now you can use the 'data' list as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlq-ZGz6_6Q-"
   },
   "source": [
    "Let's check the total number of rows in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:08.478995Z",
     "iopub.status.busy": "2024-10-27T14:18:08.478616Z",
     "iopub.status.idle": "2024-10-27T14:18:08.485776Z",
     "shell.execute_reply": "2024-10-27T14:18:08.484831Z",
     "shell.execute_reply.started": "2024-10-27T14:18:08.478966Z"
    },
    "id": "MZTScnw3_6Q-",
    "outputId": "1595aaf0-0e8e-4dea-84de-6bb03b4cebd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45863"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:10.677608Z",
     "iopub.status.busy": "2024-10-27T14:18:10.676921Z",
     "iopub.status.idle": "2024-10-27T14:18:10.683541Z",
     "shell.execute_reply": "2024-10-27T14:18:10.682601Z",
     "shell.execute_reply.started": "2024-10-27T14:18:10.677574Z"
    },
    "id": "KyMznPjt_6Q-",
    "outputId": "a17016d8-02fe-4aab-8e0b-9640ba45a6b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Question:\\nWhat actions can local governments take to promote carbon emissions footprint reductions?\\n\\nAnswer:\\nLocal governments can promote carbon footprint reduction by implementing policies that enhance **shared transport**, \\nsupport **renewable energy**, and encourage community engagement.',\n",
       " 'Question:\\nWhere can I access a calculator for determining my FLIGHTS carbon emissions footprint?\\n\\nAnswer:\\nYes, you can estimate your flight pollution using this calculator: [Flight Carbon Footprint Calculator](https://www.atag.org/our-activities/environmental-initiatives/carbon-footprint-calculator.html)',\n",
       " 'Question:\\nWhat effects does plastic waste have on carbon release?\\n\\nAnswer:\\nPlastic waste significantly contributes to **carbon toxic emissions** during its production and degradation phases, further exacerbated by improper disposal methods leading to environmental pollution.',\n",
       " \"Question:\\nWhat does the abbreviation 'TNC' represent in the context of climate emergency?\\n\\nAnswer:\\nTRANSNATIONAL CORPORATION:\\nA company that operates in multiple countries, often influencing global carbon discharge.\",\n",
       " 'Question:\\nHow can we reduce carbon footprints using sustainable energy, and what technology helps reduce greenhouse gases?\\n\\nAnswer:\\nRenewable energy sources like **solar** and **wind power** do not produce carbon gas emissions during electricity generation. Transitioning to these sources can significantly reduce our **carbon footprint**.',\n",
       " 'Question:\\nHow do waste reduction strategies practices directly influence carbon emissions?\\n\\nAnswer:\\n**Waste management practices** greatly affect carbon emissions; \\nmethods such as **composting** and **reprocessing** are typically less carbon-intensive compared to **landfilling**.',\n",
       " 'Question:\\nHow do common household items vary in their carbon mark?\\n\\nAnswer:\\nCommon **household items** have differing carbon emissions footprints; \\nelectronics and appliances are notable for significant emissions from their production and disposal processes.',\n",
       " \"Question:\\nWhat is ecological footprint and how can we reduce it? How can we reduce our climate footprint in nature?\\n\\nAnswer:\\nAn **ecological footprint** measures how much **natural resources** a human being consumes against the earth's ability to regenerate them. Reducing **carbon footprint** can help heal the environment and mitigate **climatic shift**.\",\n",
       " 'Question:\\nWhat are the research topics related to climate footprint research in electrical and computer engineering?\\n\\nAnswer:\\nThere are several **research topics** related to carbon impact in electrical and computer engineering, including:\\n- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\\n- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\\n- **Management** of loading and unloading of **thermal solar energy storage systems** for electric generation plants.',\n",
       " 'Question:\\nCan we realistically save the world from global warming?\\n\\nAnswer:\\nYes, but only if we all act now and fast.\\nKeeping warming to **1.5°C** is still possible with international cooperation\\nto make deep, rapid cuts to greenhouse gas discharge.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "213HQ3UM_6Q-"
   },
   "source": [
    "For easiness, we will create the following template for QA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMiAJ7Ja_6Q_"
   },
   "source": [
    "## Template utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:14.509260Z",
     "iopub.status.busy": "2024-10-27T14:18:14.508529Z",
     "iopub.status.idle": "2024-10-27T14:18:14.514337Z",
     "shell.execute_reply": "2024-10-27T14:18:14.513362Z",
     "shell.execute_reply.started": "2024-10-27T14:18:14.509228Z"
    },
    "id": "coXISIgy_6Q_"
   },
   "outputs": [],
   "source": [
    "def colorize_text(text):\n",
    "    for word, color in zip([\"Question\", \"Answer\"], [\"red\", \"green\"]):\n",
    "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxhVnWMi_6Q_"
   },
   "source": [
    "# Specialized class to query Gemma\n",
    "\n",
    "\n",
    "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH459kgO_6Q_"
   },
   "source": [
    "## Initialize the code for Gemma Causal LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:18:17.901163Z",
     "iopub.status.busy": "2024-10-27T14:18:17.900461Z",
     "iopub.status.idle": "2024-10-27T14:19:18.446915Z",
     "shell.execute_reply": "2024-10-27T14:19:18.446045Z",
     "shell.execute_reply.started": "2024-10-27T14:18:17.901128Z"
    },
    "id": "o9zy-S2c_6Q_",
    "outputId": "10ecdfad-99af-4396-f7e9-9aff08ddefed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7q0z5KO_6RA"
   },
   "source": [
    "## Define the specialized class\n",
    "\n",
    "Here we define the special class `GemmaQA`.\n",
    "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
    "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.448750Z",
     "iopub.status.busy": "2024-10-27T14:19:18.448493Z",
     "iopub.status.idle": "2024-10-27T14:19:18.454865Z",
     "shell.execute_reply": "2024-10-27T14:19:18.453940Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.448726Z"
    },
    "id": "M29bWaCj_6RA"
   },
   "outputs": [],
   "source": [
    "class GemmaQA:\n",
    "    def __init__(self, max_length=512):\n",
    "        self.max_length = max_length\n",
    "        self.prompt = template\n",
    "        self.gemma_causal_lm = gemma_causal_lm\n",
    "\n",
    "    def query(self, question):\n",
    "        response = self.gemma_causal_lm.generate(\n",
    "            self.prompt.format(\n",
    "                question=question,\n",
    "                answer=\"\"),\n",
    "            max_length=self.max_length)\n",
    "        display(Markdown(colorize_text(response)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgPJYElj_6RA"
   },
   "source": [
    "## Gemma preprocessor\n",
    "\n",
    "\n",
    "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
    "\n",
    "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.456252Z",
     "iopub.status.busy": "2024-10-27T14:19:18.455985Z",
     "iopub.status.idle": "2024-10-27T14:19:18.583614Z",
     "shell.execute_reply": "2024-10-27T14:19:18.582827Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.456228Z"
    },
    "id": "8tJMgskc_6RA"
   },
   "outputs": [],
   "source": [
    "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.585959Z",
     "iopub.status.busy": "2024-10-27T14:19:18.585646Z",
     "iopub.status.idle": "2024-10-27T14:19:18.592121Z",
     "shell.execute_reply": "2024-10-27T14:19:18.591229Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.585918Z"
    },
    "id": "Iu-r-Ni0_6RA",
    "outputId": "18c31f13-5a68-4866-e486-bca97ad8c312"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'token_ids': Array([[     2,   9413, 235292, ...,      0,      0,      0],\n",
      "       [     2,   9413, 235292, ...,      0,      0,      0]],      dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
      "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[  9413 235292    108 ...      0      0      0]\n",
      " [  9413 235292    108 ...      0      0      0]]\n"
     ]
    }
   ],
   "source": [
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-njmLjeG_6RB"
   },
   "source": [
    "# Perform fine-tuning with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVWHm6uT_6RB"
   },
   "source": [
    "## Enable LoRA for the model\n",
    "\n",
    "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:18.593603Z",
     "iopub.status.busy": "2024-10-27T14:19:18.593323Z",
     "iopub.status.idle": "2024-10-27T14:19:19.097275Z",
     "shell.execute_reply": "2024-10-27T14:19:19.096429Z",
     "shell.execute_reply.started": "2024-10-27T14:19:18.593578Z"
    },
    "id": "4QgAbDaT_6RB",
    "outputId": "f6defce3-38fa-42ec-99af-a9de1c96f18e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
       "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,734,848</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,618,734,848\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,734,848</span> (9.76 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,618,734,848\u001b[0m (9.76 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,392,960</span> (16.76 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,392,960\u001b[0m (16.76 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n",
    "gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n",
    "gemma_causal_lm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aREx_T86_6RB"
   },
   "source": [
    "We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only ~4.39 Millions parameters trainable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XaY8X3D_6RC"
   },
   "source": [
    "## Run the training sequence\n",
    "\n",
    "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
    "We compile the model, with the loss, optimizer and metric.\n",
    "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-27T14:19:19.098736Z",
     "iopub.status.busy": "2024-10-27T14:19:19.098440Z",
     "iopub.status.idle": "2024-10-28T00:51:45.441130Z",
     "shell.execute_reply": "2024-10-28T00:51:45.440190Z",
     "shell.execute_reply.started": "2024-10-27T14:19:19.098710Z"
    },
    "id": "DK7b1PSM_6RC",
    "outputId": "c75b3625-f24a-4548-fb08-3d1242211dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m11466/11466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3101s\u001b[0m 266ms/step - loss: 0.1052 - sparse_categorical_accuracy: 0.7950\n",
      "Epoch 2/5\n",
      "\u001b[1m11466/11466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3008s\u001b[0m 261ms/step - loss: 0.0258 - sparse_categorical_accuracy: 0.9362\n",
      "Epoch 3/5\n",
      "\u001b[1m11466/11466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2990s\u001b[0m 261ms/step - loss: 0.0239 - sparse_categorical_accuracy: 0.9377\n",
      "Epoch 4/5\n",
      "\u001b[1m11466/11466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2991s\u001b[0m 261ms/step - loss: 0.0231 - sparse_categorical_accuracy: 0.9383\n",
      "Epoch 5/5\n",
      "\u001b[1m11466/11466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2990s\u001b[0m 261ms/step - loss: 0.0226 - sparse_categorical_accuracy: 0.9386\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7fd658672740>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set sequence length cf. config (512)\n",
    "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length\n",
    "\n",
    "# Compile the model with loss, optimizer, and metric\n",
    "gemma_causal_lm.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
    "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# Train model\n",
    "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGiAZrLU_6RC"
   },
   "source": [
    "# Test the fine-tuned model\n",
    "\n",
    "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.442525Z",
     "iopub.status.busy": "2024-10-28T00:51:45.442251Z",
     "iopub.status.idle": "2024-10-28T00:51:45.446749Z",
     "shell.execute_reply": "2024-10-28T00:51:45.445834Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.442501Z"
    },
    "id": "Ifb4R7Em_6RC"
   },
   "outputs": [],
   "source": [
    "gemma_qa = GemmaQA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B98EFX7O_6RD"
   },
   "source": [
    "For start, we are testing the model with some of the data from the training set itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7BRI-TC_6RD"
   },
   "source": [
    "## Sample 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.448282Z",
     "iopub.status.busy": "2024-10-28T00:51:45.447832Z",
     "iopub.status.idle": "2024-10-28T00:51:45.778444Z",
     "shell.execute_reply": "2024-10-28T00:51:45.777544Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.448256Z"
    },
    "id": "NZDGhLQc_6RD",
    "outputId": "13f1aff7-d259-4669-9f49-071e9e37fcae"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"df\",\n  \"rows\": 45863,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3295,\n        \"samples\": [\n          \"What are some notable sources of regenerative energy?\",\n          \"Why is using mass transit often more sustainable than private vehicle usage?\",\n          \"How are **heat-trapping gases** measured, estimated, and reported?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4032,\n        \"samples\": [\n          \"Improving **energy efficiency** in appliances can lead to substantial reductions in electricity use, significantly lowering carbon gas emissions \\nat both individual and collective levels.\",\n          \"You should refer to reliable and quality-assured sources of environmental change science,\\nsuch as **peer-reviewed papers**.\",\n          \"Carbon footprint audits are essential for organizations to identify significant sources of toxic emissions, \\nallowing them to create targeted strategies for reduction.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-3c4cd761-bdeb-4e4e-9b82-09dda04684f7\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What actions can local governments take to pro...</td>\n",
       "      <td>Local governments can promote carbon footprint...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Where can I access a calculator for determinin...</td>\n",
       "      <td>Yes, you can estimate your flight pollution us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What effects does plastic waste have on carbon...</td>\n",
       "      <td>Plastic waste significantly contributes to **c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the abbreviation 'TNC' represent in ...</td>\n",
       "      <td>TRANSNATIONAL CORPORATION:\\nA company that ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we reduce carbon footprints using sust...</td>\n",
       "      <td>Renewable energy sources like **solar** and **...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c4cd761-bdeb-4e4e-9b82-09dda04684f7')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-3c4cd761-bdeb-4e4e-9b82-09dda04684f7 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-3c4cd761-bdeb-4e4e-9b82-09dda04684f7');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-7800a7c1-ecb6-4199-88e4-380ab2f1a730\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7800a7c1-ecb6-4199-88e4-380ab2f1a730')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-7800a7c1-ecb6-4199-88e4-380ab2f1a730 button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What actions can local governments take to pro...   \n",
       "1  Where can I access a calculator for determinin...   \n",
       "2  What effects does plastic waste have on carbon...   \n",
       "3  What does the abbreviation 'TNC' represent in ...   \n",
       "4  How can we reduce carbon footprints using sust...   \n",
       "\n",
       "                                              answer  \n",
       "0  Local governments can promote carbon footprint...  \n",
       "1  Yes, you can estimate your flight pollution us...  \n",
       "2  Plastic waste significantly contributes to **c...  \n",
       "3  TRANSNATIONAL CORPORATION:\\nA company that ope...  \n",
       "4  Renewable energy sources like **solar** and **...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df =  pd.read_csv(\"/content/drive/MyDrive/Hungers_data/format1.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.780186Z",
     "iopub.status.busy": "2024-10-28T00:51:45.779810Z",
     "iopub.status.idle": "2024-10-28T00:51:45.787120Z",
     "shell.execute_reply": "2024-10-28T00:51:45.786216Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.780154Z"
    },
    "id": "vM4Ps0ZC_6RM",
    "outputId": "d2951e07-43ea-4c2c-df45-0c936acc7080"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>question</th>\n",
       "      <td>What does production of **meat and dairy produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>answer</th>\n",
       "      <td>Livestock production contributes to climate ch...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> object</label>"
      ],
      "text/plain": [
       "question    What does production of **meat and dairy produ...\n",
       "answer      Livestock production contributes to climate ch...\n",
       "Name: 15, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:51:45.790818Z",
     "iopub.status.busy": "2024-10-28T00:51:45.790537Z",
     "iopub.status.idle": "2024-10-28T00:52:08.880613Z",
     "shell.execute_reply": "2024-10-28T00:52:08.879680Z",
     "shell.execute_reply.started": "2024-10-28T00:51:45.790795Z"
    },
    "id": "MIxQ65cq_6RN",
    "outputId": "1e81ba0f-0beb-4291-dc32-a2329f8fb648"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What does production of **meat and dairy products** have to do with climate disruption?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Livestock production contributes to climate change through:\n",
       "- **Methane outflow**\n",
       "- **Deforestation** for animal agriculture,\n",
       "accounting for a significant percentage of **global greenhouse gas outflow**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[15]\n",
    "gemma_qa.query(row.loc[\"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:08.882368Z",
     "iopub.status.busy": "2024-10-28T00:52:08.882060Z",
     "iopub.status.idle": "2024-10-28T00:52:31.257467Z",
     "shell.execute_reply": "2024-10-28T00:52:31.256487Z",
     "shell.execute_reply.started": "2024-10-28T00:52:08.882343Z"
    },
    "id": "wtMs-8uT_6RN",
    "outputId": "6b9c2cf1-a3ed-4187-951c-1a622fb7e162"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Other than carbon foot print name other foot print?\n",
      "\n",
      "Answer:\n",
      "Yes, there are other environmental footprints in addition to **carbon airborne pollutants**, such as **environmental footprint** and **ecological footprint**.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"Other than carbon foot print name other foot print?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:31.259449Z",
     "iopub.status.busy": "2024-10-28T00:52:31.258999Z",
     "iopub.status.idle": "2024-10-28T00:52:32.347737Z",
     "shell.execute_reply": "2024-10-28T00:52:32.346774Z",
     "shell.execute_reply.started": "2024-10-28T00:52:31.259414Z"
    },
    "id": "DYeXVQxF_6RO",
    "outputId": "34b4d688-faf5-4dfc-afff-2bc2e61b0455"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "Who are you?\n",
      "\n",
      "Answer:\n",
      "I'm a college student studying sustainability and climate variability. My research focuses on the effects of tourism development on carbon legacys in coastal areas.\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"Who are you?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:32.349577Z",
     "iopub.status.busy": "2024-10-28T00:52:32.349267Z",
     "iopub.status.idle": "2024-10-28T00:52:33.814310Z",
     "shell.execute_reply": "2024-10-28T00:52:33.813366Z",
     "shell.execute_reply.started": "2024-10-28T00:52:32.349549Z"
    },
    "id": "C2yYOE-d_6RO",
    "outputId": "424057a7-9720-4415-ae3f-66b4d2fb6b6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "List two types of caron foot prints?\n",
      "\n",
      "Answer:\n",
      "Common ways to reduce **carbon emissions** include:\n",
      "- **Driving less**\n",
      "- **Using public transport**\n"
     ]
    }
   ],
   "source": [
    "prompt = template.format(\n",
    "    question=\"List two types of caron foot prints?\",\n",
    "    answer=\"\",\n",
    ")\n",
    "print(gemma_causal_lm.generate(prompt, max_length=256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5y9XcLL4_6RO"
   },
   "source": [
    "## Sample 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:33.817300Z",
     "iopub.status.busy": "2024-10-28T00:52:33.815534Z",
     "iopub.status.idle": "2024-10-28T00:52:37.474231Z",
     "shell.execute_reply": "2024-10-28T00:52:37.473263Z",
     "shell.execute_reply.started": "2024-10-28T00:52:33.817270Z"
    },
    "id": "E9l2RMgm_6RP",
    "outputId": "812b80a1-946e-4fff-8a6c-1729cd6ff3b0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What are the research topics related to climate footprint research in electrical and computer engineering?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "There are several **research topics** related to carbon footprint in electrical and computer engineering, including:\n",
       "- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\n",
       "- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\n",
       "- **Management** of loading and unloading of **thermal solar energy storage systems** for electric generation plants."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[8]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcVv5elL_6RP"
   },
   "source": [
    "## Sample 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:37.476018Z",
     "iopub.status.busy": "2024-10-28T00:52:37.475669Z",
     "iopub.status.idle": "2024-10-28T00:52:39.558912Z",
     "shell.execute_reply": "2024-10-28T00:52:39.558022Z",
     "shell.execute_reply.started": "2024-10-28T00:52:37.475987Z"
    },
    "id": "Qxw9J8Re_6RP",
    "outputId": "33c1ab84-838c-4d77-ae52-17ba03eb3b3f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "Which energy resource has the smallest climate footprint?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "**Wind, nuclear, tidal, hydropower, geothermal, solar,** and **wave energy** have the lowest carbon legacys, emitting between **11 and 48 gCO2** on a life-cycle basis."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row = df.iloc[25]\n",
    "gemma_qa.query(row.question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSruif-T_6RQ"
   },
   "source": [
    "## Not seen question(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:39.560350Z",
     "iopub.status.busy": "2024-10-28T00:52:39.560064Z",
     "iopub.status.idle": "2024-10-28T00:52:41.563088Z",
     "shell.execute_reply": "2024-10-28T00:52:41.562090Z",
     "shell.execute_reply.started": "2024-10-28T00:52:39.560325Z"
    },
    "id": "fBAayx6e_6RQ",
    "outputId": "62f53db0-9538-4504-d17c-c624a5616f15"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What is Climate Change?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Climate change refers to the long-term alteration of temperature and typical weather patterns in a place,\n",
       "primarily driven by human activities that increase air pollutants in the atmosphere,\n",
       "such as burning fossil fuels and deforestation."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is Climate Change?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:41.564380Z",
     "iopub.status.busy": "2024-10-28T00:52:41.564116Z",
     "iopub.status.idle": "2024-10-28T00:52:44.038894Z",
     "shell.execute_reply": "2024-10-28T00:52:44.037926Z",
     "shell.execute_reply.started": "2024-10-28T00:52:41.564357Z"
    },
    "id": "xr5184Pt_6RQ",
    "outputId": "113b8b85-329a-4ee0-8e9a-fac605bb6dd0"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "List ways to reduce carbon foot print?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Effective strategies for reducing **carbon emissions footprints** include: \n",
       "   - **using public transportation** \n",
       "   - **embracing a plant-based diet** \n",
       "   - **reducing energy consumption at home** \n",
       "   - **advocating for renewable energy sources**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"List ways to reduce carbon foot print?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:44.040578Z",
     "iopub.status.busy": "2024-10-28T00:52:44.040216Z",
     "iopub.status.idle": "2024-10-28T00:52:45.492706Z",
     "shell.execute_reply": "2024-10-28T00:52:45.491752Z",
     "shell.execute_reply.started": "2024-10-28T00:52:44.040544Z"
    },
    "id": "NLCRZAO-_6RR",
    "outputId": "21174c26-e46f-4b79-c87f-bc8457cda3cf"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What is a code competition?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "A **carbon reduction competition** is a challenge between organizations to see which can reduce their carbon mark below a given threshold first."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is a code competition?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:45.494134Z",
     "iopub.status.busy": "2024-10-28T00:52:45.493831Z",
     "iopub.status.idle": "2024-10-28T00:52:47.771201Z",
     "shell.execute_reply": "2024-10-28T00:52:47.770302Z",
     "shell.execute_reply.started": "2024-10-28T00:52:45.494107Z"
    },
    "id": "tlciGP2Q_6RR",
    "outputId": "a0847ac8-ae77-4270-ee9e-5122cac2be24"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What are the steps to create a Kaggle dataset?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Steps to create a **data set** include:\n",
       "- **Acquisition** of data\n",
       "- **Cleansing** and **standardization**\n",
       "- **Splitting** into **training** and **testing** sets\n",
       "- **Labeling** as **required**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What are the steps to create a Kaggle dataset?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:58:51.062524Z",
     "iopub.status.busy": "2024-10-28T00:58:51.062098Z",
     "iopub.status.idle": "2024-10-28T00:58:53.391249Z",
     "shell.execute_reply": "2024-10-28T00:58:53.390320Z",
     "shell.execute_reply.started": "2024-10-28T00:58:51.062486Z"
    },
    "id": "GChpb3gD_6RR",
    "outputId": "6c362304-877d-4d0d-ce2d-3142a80b468f"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "How can I reduce my carbon foot print as a Multimedia University of Kenya Student?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "I need to gather more **information** about carbon legacings to **respond** effectively."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How can I reduce my carbon foot print as a Multimedia University of Kenya Student?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:59:30.071082Z",
     "iopub.status.busy": "2024-10-28T00:59:30.070724Z",
     "iopub.status.idle": "2024-10-28T00:59:31.141327Z",
     "shell.execute_reply": "2024-10-28T00:59:31.140343Z",
     "shell.execute_reply.started": "2024-10-28T00:59:30.071055Z"
    },
    "id": "ak9nC9z4_6RS",
    "outputId": "f2a7aa83-05be-44d5-fbed-2e8065497f2b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "How Old are you?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "I'm sorry, I don't understand your question."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How Old are you?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "id": "6M-LlsjNXuMU",
    "outputId": "70616958-4f68-4c4a-aa26-a1fbe3b25c10"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "How much carbon does an indivudual produce daily?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "On an average, an individual produces about **1 kg** of carbon release per day, which includes emissions from: \n",
       "   - **consumer activities** \n",
       "   - **production processes**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"How much carbon does an indivudual produce daily?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "GBGyB_W8YGRz",
    "outputId": "cd63534d-a9e8-4323-bfd2-2b597250612b"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "What is water foot print?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Water footprint measures the amount of water needed in a product life cycle,\n",
       "which can be used to assess how products affect water supply."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"What is water foot print?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "JRghv4TrYuV2",
    "outputId": "bc9b73b9-1a3b-4545-db07-2c454974adb8"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "Can water foot print lead to carbon foot print?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Yes, **water footprint** can lead to **carbon carbon releases** if water usage is inefficient, which increases carbon releases associated with water resource usage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"Can water foot print lead to carbon foot print?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "xbBmkrcUY5aM",
    "outputId": "d00f21d3-8aec-435a-ea77-c56f4e68f825"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Question:\n",
       "what should I do when I wake up?\n",
       "\n",
       "**<font color='green'>Answer:</font>**\n",
       "Start your day with a warm glass of water to hydrate your body and improve digestion. Avoid heavy or acidic foods that can upset your stomach."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "question = \"what should I do when I wake up?\"\n",
    "gemma_qa.query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RDca_gCU_6RS"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-28T00:52:47.772712Z",
     "iopub.status.busy": "2024-10-28T00:52:47.772416Z",
     "iopub.status.idle": "2024-10-28T00:53:23.227706Z",
     "shell.execute_reply": "2024-10-28T00:53:23.226867Z",
     "shell.execute_reply.started": "2024-10-28T00:52:47.772686Z"
    },
    "id": "DKPF0hyA_6RT"
   },
   "outputs": [],
   "source": [
    "preset_dir = \".\\gemma2_2b_en_MNLR6_model_docs\"\n",
    "gemma_causal_lm.save_to_preset(preset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShoNb5-P_6RT"
   },
   "source": [
    "# Publish Model on Kaggle as a Kaggle Model\n",
    "\n",
    "We are publishing now the saved model as a Kaggle Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-10-28T00:53:23.229084Z",
     "iopub.status.busy": "2024-10-28T00:53:23.228802Z",
     "iopub.status.idle": "2024-10-28T00:56:36.360074Z",
     "shell.execute_reply": "2024-10-28T00:56:36.359114Z",
     "shell.execute_reply.started": "2024-10-28T00:53:23.229058Z"
    },
    "id": "Ev5hYdXs_6RU",
    "outputId": "b9a42ad8-37e9-448b-f499-3739492bfdd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading Model https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR6_model_docs ...\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/task.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|██████████| 2.98k/2.98k [00:00<00:00, 3.78kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/task.json (3KB)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/tokenizer.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 591/591 [00:00<00:00, 751B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/tokenizer.json (591B)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/preprocessor.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 1.41k/1.41k [00:00<00:00, 1.83kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/preprocessor.json (1KB)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/metadata.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 143/143 [00:00<00:00, 187B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/metadata.json (143B)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/model.weights.h5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 10.5G/10.5G [07:17<00:00, 23.9MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/model.weights.h5 (10GB)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 782/782 [00:00<00:00, 973B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/config.json (782B)\n",
      "Starting upload for file .\\gemma2_2b_en_MNLR6_model_docs/assets/tokenizer/vocabulary.spm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Uploading: 100%|██████████| 4.24M/4.24M [00:02<00:00, 1.70MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upload successful: .\\gemma2_2b_en_MNLR6_model_docs/assets/tokenizer/vocabulary.spm (4MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your model instance has been created.\n",
      "Files are being processed...\n",
      "See at: https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR6_model_docs\n"
     ]
    }
   ],
   "source": [
    "kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n",
    "\n",
    "kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR6_model_docs\"\n",
    "keras_nlp.upload_preset(kaggle_uri, preset_dir)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4484051,
     "sourceId": 7711309,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5853991,
     "sourceId": 9596582,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5958248,
     "sourceId": 9735365,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5958454,
     "sourceId": 9735634,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 78150,
     "modelInstanceId": 72244,
     "sourceId": 85984,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
