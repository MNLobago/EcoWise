{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "JbLd6pUB_6Qw"
      },
      "source": [
        "<center><h1>Fine-tuning Gemma 2 model using LoRA and Keras</h1></center>\n",
        "\n",
        "<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n",
        "\n",
        "\n",
        "# Introduction\n",
        "\n",
        "This notebook will demonstrate three things:\n",
        "\n",
        "1. How to fine-tune Gemma model using LoRA\n",
        "2. Creation of a specialised class to query about Kaggle features\n",
        "3. Some results of querying about Kaggle Docs\n",
        "\n",
        "This work is largely based on previous work. Here I list the sources:\n",
        "\n",
        "1. Gemma 2 Model Card, Kaggle Models,https://www.kaggle.com/models/google/gemma-2/\n",
        "2. Kaggle QA with Gemma - KerasNLP Starter, Kaggle Code, https://www.kaggle.com/code/awsaf49/kaggle-qa-with-gemma-kerasnlp-starter (Version 11)  \n",
        "3. Fine-tune Gemma models in Keras using LoRA, Kaggle Code, https://www.kaggle.com/code/nilaychauhan/fine-tune-gemma-models-in-keras-using-lora (Version 1)\n",
        "4. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, LoRA: Low-Rank Adaptation of Large Language Models, ArXiv, https://arxiv.org/pdf/2106.09685.pdf\n",
        "5. Abheesht Sharma, Matthew Watson, Parameter-efficient fine-tuning of GPT-2 with LoRA, https://keras.io/examples/nlp/parameter_efficient_finetuning_of_gpt2_with_lora/\n",
        "6. Keras 3 API documentation / KerasNLP / Models / Gemma, https://keras.io/api/keras_nlp/models/gemma/\n",
        "7. Unlock the Power of Gemma 2: Prompt it like a Pro, https://www.kaggle.com/code/gpreda/unlock-the-power-of-gemma-2-prompt-it-like-a-pro  \n",
        "8. Fine-tune Gemma using LoRA and Keras, https://www.kaggle.com/code/gpreda/fine-tune-gemma-using-lora-and-keras\n",
        "9. Fine-tunning Gemma model with Kaggle Docs data, https://www.kaggle.com/code/gpreda/fine-tunning-gemma-model-with-kaggle-docs-data\n",
        "10. Kaggle Docs, Kaggle Dataset, https://www.kaggle.com/datasets/awsaf49/kaggle-docs  \n",
        "\n",
        "\n",
        "**Let's go**!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVbdQU09_6Q1"
      },
      "source": [
        "# What is Gemma 2?\n",
        "\n",
        "Gemma is a collection of lightweight, advanced open models developed by Google, leveraging the same research and technology behind the Gemini models. These models are text-to-text, decoder-only large language models available in English, with open weights provided for both pre-trained and instruction-tuned versions. Gemma models excel in a range of text generation tasks, such as question answering, summarization, and reasoning. Their compact size allows for deployment in resource-constrained environments like laptops, desktops, or personal cloud infrastructure, making state-of-the-art AI models more accessible and encouraging innovation for all.\n",
        "\n",
        "Gemma 2 represent the 2nd generation of Gemma models. These models were trained on a dataset of text data that includes a wide variety of sources. The **27B** model was trained with **13 trillion** tokens, the **9B** model was trained with **8 trillion tokens**, and **2B** model was trained with **2 trillion** tokens. Here is a summary of their key components:\n",
        "* **Web Documents**: A diverse collection of web text ensures the model is exposed to a broad range of linguistic styles, topics, and vocabulary. Primarily English-language content.\n",
        "* **Code**: Exposing the model to code helps it to learn the syntax and patterns of programming languages, which improves its ability to generate code or understand code-related questions.\n",
        "* **Mathematics**: Training on mathematical text helps the model learn logical reasoning, symbolic representation, and to address mathematical queries.\n",
        "\n",
        "To learn more about Gemma 2, follow this link: [Gemma 2 Model Card](https://www.kaggle.com/models/google/gemma-2).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WbkaQaV_6Q2"
      },
      "source": [
        "# What is LoRA?  \n",
        "\n",
        "**LoRA** stands for **Low-Rank Adaptation**. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to **LoRA** paper, this number decreases **10,000 times**, and the computational resources size decreases 3 times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hb1Yfw5B_6Q2"
      },
      "source": [
        "# How we proceed?\n",
        "\n",
        "For fine-tunning with LoRA, we will follow the steps:\n",
        "\n",
        "1. Install prerequisites\n",
        "2. Load and process the data for fine-tuning\n",
        "3. Initialize the code for Gemma causal language model (Gemma Causal LM)\n",
        "4. Perform fine-tuning\n",
        "5. Test the fine-tunned model with questions from the data used for fine-tuning and with aditional questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqykmF-v_6Q3"
      },
      "source": [
        "# Prerequisites\n",
        "\n",
        "\n",
        "## Install packages\n",
        "\n",
        "We start by installing `keras-nlp` and `keras` packages."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9we3AQKiKpB0",
        "outputId": "74aa1e9a-94d6-465a-80ba-48adbea14c14"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Nov  3 14:29:10 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              41W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#/content/drive/MyDrive/Hungers_data"
      ],
      "metadata": {
        "id": "lfXg_CrnDOUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSB-y51BI4c",
        "outputId": "cd15b5cd-f778-4d21-8cfe-c9423b829024"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle  # Create the .kaggle directory if it doesn't exist\n",
        "!cp /content/drive/MyDrive/Hungers_data/kaggle.json ~/.kaggle/  # Copy the file\n",
        "!chmod 600 ~/.kaggle/kaggle.json  # Set permissions for the file"
      ],
      "metadata": {
        "id": "SM81nR-lDRg1"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "# Load kaggle.json from Google Drive\n",
        "with open('/content/drive/MyDrive/Hungers_data/kaggle.json') as f:\n",
        "    userdata = json.load(f)  # Load the JSON data\n",
        "\n",
        "# Set environment variables\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('username')  # Extract username\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('key')            # Extract key\n",
        "\n",
        "# Verify that the environment variables are set\n",
        "#print(\"KAGGLE_USERNAME:\", os.environ[\"KAGGLE_USERNAME\"])\n",
        "#print(\"KAGGLE_KEY:\", os.environ[\"KAGGLE_KEY\"])"
      ],
      "metadata": {
        "id": "Dm9z1de6GH0Q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle models download -m keras/gemma"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7HWtpjeDlgX",
        "outputId": "3253aaf4-7858-42d4-8e8b-a0d3afec3b79"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle models [-h] {instances,get,list,init,create,delete,update} ...\n",
            "kaggle models: error: argument command: invalid choice: 'download' (choose from 'instances', 'get', 'list', 'init', 'create', 'delete', 'update')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-10-27T14:13:38.123623Z",
          "iopub.status.busy": "2024-10-27T14:13:38.123157Z",
          "iopub.status.idle": "2024-10-27T14:14:21.265319Z",
          "shell.execute_reply": "2024-10-27T14:14:21.264120Z",
          "shell.execute_reply.started": "2024-10-27T14:13:38.123595Z"
        },
        "id": "P-klIBWD_6Q3"
      },
      "outputs": [],
      "source": [
        "# Install Keras 3 last. See https://keras.io/getting_started/ for more details.\n",
        "!pip install -q -U keras-nlp\n",
        "!pip install -q -U keras>=3\n",
        "!pip install -q -U kagglehub --upgrade"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcMr8E19_6Q7"
      },
      "source": [
        "## Import packages\n",
        "\n",
        "Now we can import the packages we just installed. We will also install `os`, so that we can set the environment variables needed for keras backend. We will use `jax` as `KERAS_BACKEND`.\n",
        "\n",
        "Because we want to publish the Model from the Notebook, we also include `kagglehub` and import secrets from `Kaggle App`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "_kg_hide-output": true,
        "execution": {
          "iopub.execute_input": "2024-10-27T14:17:43.408556Z",
          "iopub.status.busy": "2024-10-27T14:17:43.407386Z",
          "iopub.status.idle": "2024-10-27T14:17:59.147958Z",
          "shell.execute_reply": "2024-10-27T14:17:59.146923Z",
          "shell.execute_reply.started": "2024-10-27T14:17:43.408514Z"
        },
        "id": "-5Kkx8Cp_6Q8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\" # you can also use tensorflow or torch\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.00\" # avoid memory fragmentation on JAX backend.\n",
        "os.environ[\"JAX_PLATFORMS\"] = \"\"\n",
        "import keras\n",
        "import keras_nlp\n",
        "import kagglehub\n",
        "\n",
        "\n",
        "#from kaggle_secrets import UserSecretsClient\n",
        "#user_secrets = UserSecretsClient()\n",
        "#os.environ[\"KAGGLE_USERNAME\"] = user_secrets.get_secret(\"kaggle_username\")\n",
        "#os.environ[\"KAGGLE_KEY\"] = user_secrets.get_secret(\"kaggle_key\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm\n",
        "tqdm.pandas() # progress bar for pandas\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVcSHLL6_6Q8"
      },
      "source": [
        "## Configurations\n",
        "\n",
        "\n",
        "We use a `Config` class to group the information needed to control the fine-tuning process:\n",
        "* random seed\n",
        "* dataset path\n",
        "* preset - name of pretrained Gemma 2\n",
        "* sequence length - this is the maximum size of input sequence for training\n",
        "* batch size - size of the input batch in training, x 2 as two GPUs\n",
        "* lora rank - rank for LoRA, higher means more trainable parameters\n",
        "* learning rate used in the train\n",
        "* epochs - number of epochs for train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:17:59.150496Z",
          "iopub.status.busy": "2024-10-27T14:17:59.149796Z",
          "iopub.status.idle": "2024-10-27T14:17:59.156214Z",
          "shell.execute_reply": "2024-10-27T14:17:59.155284Z",
          "shell.execute_reply.started": "2024-10-27T14:17:59.150451Z"
        },
        "id": "iPh1I2gK_6Q8"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    seed = 42\n",
        "    dataset_path = \"/content/drive/MyDrive/Hungers_data\"\n",
        "    preset = \"gemma2_2b_en\" # name of pretrained Gemma 2\n",
        "    sequence_length = 512 # max size of input sequence for training\n",
        "    batch_size = 7 # size of the input batch in training\n",
        "    lora_rank = 5 # rank for LoRA, higher means more trainable parameters\n",
        "    learning_rate=5e-5 # learning rate used in train\n",
        "    epochs = 3 # number of epochs to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7H-R8kq_6Q9"
      },
      "source": [
        "Set a random seed for results reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:17:59.157630Z",
          "iopub.status.busy": "2024-10-27T14:17:59.157314Z",
          "iopub.status.idle": "2024-10-27T14:17:59.167091Z",
          "shell.execute_reply": "2024-10-27T14:17:59.166295Z",
          "shell.execute_reply.started": "2024-10-27T14:17:59.157605Z"
        },
        "id": "xNmFKjfC_6Q9"
      },
      "outputs": [],
      "source": [
        "keras.utils.set_random_seed(Config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KishRbOv_6Q9"
      },
      "source": [
        "# Load the data\n",
        "\n",
        "\n",
        "We load the data we will use for fine-tunining."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:18:05.780320Z",
          "iopub.status.busy": "2024-10-27T14:18:05.779917Z",
          "iopub.status.idle": "2024-10-27T14:18:05.966861Z",
          "shell.execute_reply": "2024-10-27T14:18:05.966083Z",
          "shell.execute_reply.started": "2024-10-27T14:18:05.780291Z"
        },
        "id": "6woXxHqi_6Q9"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "data = []\n",
        "with open(f\"{Config.dataset_path}/high_volume_carbon_footprint_qa.json\") as file:\n",
        "    content = file.read()\n",
        "    try:\n",
        "        features_list = json.loads(content)\n",
        "        for features in features_list:\n",
        "            # Check if 'question' and 'answer' keys exist\n",
        "            if 'question' in features and 'answer' in features:\n",
        "                # Update the template to use the correct keys\n",
        "                template = \"Question:\\n{question}\\n\\nAnswer:\\n{answer}\"\n",
        "                data.append(template.format(**features))\n",
        "            else:\n",
        "                print(f\"Missing keys in features: {features}\")\n",
        "        #data = data[:2000]  # Limit to 1000 examples\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"Error decoding JSON: {e}\")\n",
        "\n",
        "# Now you can use the 'data' list as needed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlq-ZGz6_6Q-"
      },
      "source": [
        "Let's check the total number of rows in this dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:18:08.478995Z",
          "iopub.status.busy": "2024-10-27T14:18:08.478616Z",
          "iopub.status.idle": "2024-10-27T14:18:08.485776Z",
          "shell.execute_reply": "2024-10-27T14:18:08.484831Z",
          "shell.execute_reply.started": "2024-10-27T14:18:08.478966Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZTScnw3_6Q-",
        "outputId": "3a863998-fb07-4600-9af6-2773a8d2c617"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45863"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:18:10.677608Z",
          "iopub.status.busy": "2024-10-27T14:18:10.676921Z",
          "iopub.status.idle": "2024-10-27T14:18:10.683541Z",
          "shell.execute_reply": "2024-10-27T14:18:10.682601Z",
          "shell.execute_reply.started": "2024-10-27T14:18:10.677574Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyMznPjt_6Q-",
        "outputId": "93e5d609-be74-44df-ffa3-6b7fd3230612"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Question:\\nWhat actions can local governments take to promote carbon emissions footprint reductions?\\n\\nAnswer:\\nLocal governments can promote carbon footprint reduction by implementing policies that enhance **shared transport**, \\nsupport **renewable energy**, and encourage community engagement.',\n",
              " 'Question:\\nWhere can I access a calculator for determining my FLIGHTS carbon emissions footprint?\\n\\nAnswer:\\nYes, you can estimate your flight pollution using this calculator: [Flight Carbon Footprint Calculator](https://www.atag.org/our-activities/environmental-initiatives/carbon-footprint-calculator.html)',\n",
              " 'Question:\\nWhat effects does plastic waste have on carbon release?\\n\\nAnswer:\\nPlastic waste significantly contributes to **carbon toxic emissions** during its production and degradation phases, further exacerbated by improper disposal methods leading to environmental pollution.',\n",
              " \"Question:\\nWhat does the abbreviation 'TNC' represent in the context of climate emergency?\\n\\nAnswer:\\nTRANSNATIONAL CORPORATION:\\nA company that operates in multiple countries, often influencing global carbon discharge.\",\n",
              " 'Question:\\nHow can we reduce carbon footprints using sustainable energy, and what technology helps reduce greenhouse gases?\\n\\nAnswer:\\nRenewable energy sources like **solar** and **wind power** do not produce carbon gas emissions during electricity generation. Transitioning to these sources can significantly reduce our **carbon footprint**.',\n",
              " 'Question:\\nHow do waste reduction strategies practices directly influence carbon emissions?\\n\\nAnswer:\\n**Waste management practices** greatly affect carbon emissions; \\nmethods such as **composting** and **reprocessing** are typically less carbon-intensive compared to **landfilling**.',\n",
              " 'Question:\\nHow do common household items vary in their carbon mark?\\n\\nAnswer:\\nCommon **household items** have differing carbon emissions footprints; \\nelectronics and appliances are notable for significant emissions from their production and disposal processes.',\n",
              " \"Question:\\nWhat is ecological footprint and how can we reduce it? How can we reduce our climate footprint in nature?\\n\\nAnswer:\\nAn **ecological footprint** measures how much **natural resources** a human being consumes against the earth's ability to regenerate them. Reducing **carbon footprint** can help heal the environment and mitigate **climatic shift**.\",\n",
              " 'Question:\\nWhat are the research topics related to climate footprint research in electrical and computer engineering?\\n\\nAnswer:\\nThere are several **research topics** related to carbon impact in electrical and computer engineering, including:\\n- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\\n- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\\n- **Management** of loading and unloading of **thermal solar energy storage systems** for electric generation plants.',\n",
              " 'Question:\\nCan we realistically save the world from global warming?\\n\\nAnswer:\\nYes, but only if we all act now and fast.\\nKeeping warming to **1.5°C** is still possible with international cooperation\\nto make deep, rapid cuts to greenhouse gas discharge.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "213HQ3UM_6Q-"
      },
      "source": [
        "For easiness, we will create the following template for QA:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMiAJ7Ja_6Q_"
      },
      "source": [
        "## Template utility function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:18:14.509260Z",
          "iopub.status.busy": "2024-10-27T14:18:14.508529Z",
          "iopub.status.idle": "2024-10-27T14:18:14.514337Z",
          "shell.execute_reply": "2024-10-27T14:18:14.513362Z",
          "shell.execute_reply.started": "2024-10-27T14:18:14.509228Z"
        },
        "id": "coXISIgy_6Q_"
      },
      "outputs": [],
      "source": [
        "def colorize_text(text):\n",
        "    for word, color in zip([\"Question\", \"Answer\"], [\"red\", \"green\"]):\n",
        "        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxhVnWMi_6Q_"
      },
      "source": [
        "# Specialized class to query Gemma\n",
        "\n",
        "\n",
        "We define a specialized class to query Gemma. But first, we need to initialize an object of GemmaCausalLM class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH459kgO_6Q_"
      },
      "source": [
        "## Initialize the code for Gemma Causal LM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:18:17.901163Z",
          "iopub.status.busy": "2024-10-27T14:18:17.900461Z",
          "iopub.status.idle": "2024-10-27T14:19:18.446915Z",
          "shell.execute_reply": "2024-10-27T14:19:18.446045Z",
          "shell.execute_reply.started": "2024-10-27T14:18:17.901128Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "o9zy-S2c_6Q_",
        "outputId": "ca39c803-2eb4-43d0-b56a-b27e7e90c700"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_causal_lm = keras_nlp.models.GemmaCausalLM.from_preset(Config.preset)\n",
        "gemma_causal_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7q0z5KO_6RA"
      },
      "source": [
        "## Define the specialized class\n",
        "\n",
        "Here we define the special class `GemmaQA`.\n",
        "in the `__init__` we pass the `GemmaCausalLM` object created before.\n",
        "The `query` member function uses `GemmaCausalLM` member function `generate` to generate the answer, based on a prompt that includes the category and the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:19:18.448750Z",
          "iopub.status.busy": "2024-10-27T14:19:18.448493Z",
          "iopub.status.idle": "2024-10-27T14:19:18.454865Z",
          "shell.execute_reply": "2024-10-27T14:19:18.453940Z",
          "shell.execute_reply.started": "2024-10-27T14:19:18.448726Z"
        },
        "id": "M29bWaCj_6RA"
      },
      "outputs": [],
      "source": [
        "class GemmaQA:\n",
        "    def __init__(self, max_length=512):\n",
        "        self.max_length = max_length\n",
        "        self.prompt = template\n",
        "        self.gemma_causal_lm = gemma_causal_lm\n",
        "\n",
        "    def query(self, question):\n",
        "        response = self.gemma_causal_lm.generate(\n",
        "            self.prompt.format(\n",
        "                question=question,\n",
        "                answer=\"\"),\n",
        "            max_length=self.max_length)\n",
        "        display(Markdown(colorize_text(response)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgPJYElj_6RA"
      },
      "source": [
        "## Gemma preprocessor\n",
        "\n",
        "\n",
        "This preprocessing layer will take in batches of strings, and return outputs in a ```(x, y, sample_weight)``` format, where the y label is the next token id in the x sequence.\n",
        "\n",
        "From the code below, we can see that, after the preprocessor, the data shape is ```(num_samples, sequence_length)```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:19:18.456252Z",
          "iopub.status.busy": "2024-10-27T14:19:18.455985Z",
          "iopub.status.idle": "2024-10-27T14:19:18.583614Z",
          "shell.execute_reply": "2024-10-27T14:19:18.582827Z",
          "shell.execute_reply.started": "2024-10-27T14:19:18.456228Z"
        },
        "id": "8tJMgskc_6RA"
      },
      "outputs": [],
      "source": [
        "x, y, sample_weight = gemma_causal_lm.preprocessor(data[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:19:18.585959Z",
          "iopub.status.busy": "2024-10-27T14:19:18.585646Z",
          "iopub.status.idle": "2024-10-27T14:19:18.592121Z",
          "shell.execute_reply": "2024-10-27T14:19:18.591229Z",
          "shell.execute_reply.started": "2024-10-27T14:19:18.585918Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iu-r-Ni0_6RA",
        "outputId": "e4dcb415-f44e-4d90-a22c-2f28abae6d7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'token_ids': Array([[     2,   9413, 235292, ...,      0,      0,      0],\n",
            "       [     2,   9413, 235292, ...,      0,      0,      0]],      dtype=int32), 'padding_mask': Array([[ True,  True,  True, ..., False, False, False],\n",
            "       [ True,  True,  True, ..., False, False, False]], dtype=bool)} [[  9413 235292    108 ...      0      0      0]\n",
            " [  9413 235292    108 ...      0      0      0]]\n"
          ]
        }
      ],
      "source": [
        "print(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-njmLjeG_6RB"
      },
      "source": [
        "# Perform fine-tuning with LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVWHm6uT_6RB"
      },
      "source": [
        "## Enable LoRA for the model\n",
        "\n",
        "LoRA rank is setting the number of trainable parameters. A larger rank will result in a larger number of parameters to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:19:18.593603Z",
          "iopub.status.busy": "2024-10-27T14:19:18.593323Z",
          "iopub.status.idle": "2024-10-27T14:19:19.097275Z",
          "shell.execute_reply": "2024-10-27T14:19:19.096429Z",
          "shell.execute_reply.started": "2024-10-27T14:19:18.593578Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "4QgAbDaT_6RB",
        "outputId": "03367a74-5dc5-47c0-9ef5-b15a82cf2759"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,618,002,688\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,002,688</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,618,002,688\u001b[0m (9.75 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,618,002,688</span> (9.75 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,660,800\u001b[0m (13.96 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,660,800</span> (13.96 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Enable LoRA for the model and set the LoRA rank to the lora_rank as set in Config (4).\n",
        "gemma_causal_lm.backbone.enable_lora(rank=Config.lora_rank)\n",
        "gemma_causal_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aREx_T86_6RB"
      },
      "source": [
        "We see that only a small part of the parameters are trainable. 2.6 billions parameters total, and only 2.9 Millions parameters trainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XaY8X3D_6RC"
      },
      "source": [
        "## Run the training sequence\n",
        "\n",
        "We set the `sequence_length` for the `GemmaCausalLM` (from configuration, will be 512).\n",
        "We compile the model, with the loss, optimizer and metric.\n",
        "For the metric, it is used `SparseCategoricalAccuracy`. This metric calculates how often predictions match integer labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-27T14:19:19.098736Z",
          "iopub.status.busy": "2024-10-27T14:19:19.098440Z",
          "iopub.status.idle": "2024-10-28T00:51:45.441130Z",
          "shell.execute_reply": "2024-10-28T00:51:45.440190Z",
          "shell.execute_reply.started": "2024-10-27T14:19:19.098710Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK7b1PSM_6RC",
        "outputId": "60f62310-9776-4d26-dd43-c31fdf442d9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m6552/6552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3115s\u001b[0m 467ms/step - loss: 0.1259 - sparse_categorical_accuracy: 0.7587\n",
            "Epoch 2/3\n",
            "\u001b[1m6552/6552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3018s\u001b[0m 458ms/step - loss: 0.0270 - sparse_categorical_accuracy: 0.9353\n",
            "Epoch 3/3\n",
            "\u001b[1m6552/6552\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3000s\u001b[0m 458ms/step - loss: 0.0244 - sparse_categorical_accuracy: 0.9375\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79cad0488d60>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#set sequence length cf. config (512)\n",
        "gemma_causal_lm.preprocessor.sequence_length = Config.sequence_length\n",
        "\n",
        "# Compile the model with loss, optimizer, and metric\n",
        "gemma_causal_lm.compile(\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=Config.learning_rate),\n",
        "    weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "# Train model\n",
        "gemma_causal_lm.fit(data, epochs=Config.epochs, batch_size=Config.batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGiAZrLU_6RC"
      },
      "source": [
        "# Test the fine-tuned model\n",
        "\n",
        "We instantiate an object of class GemmaQA. Because `gemma_causal_lm` was fine-tuned using LoRA, `gemma_qa` defined here will use the fine-tuned model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:51:45.442525Z",
          "iopub.status.busy": "2024-10-28T00:51:45.442251Z",
          "iopub.status.idle": "2024-10-28T00:51:45.446749Z",
          "shell.execute_reply": "2024-10-28T00:51:45.445834Z",
          "shell.execute_reply.started": "2024-10-28T00:51:45.442501Z"
        },
        "id": "Ifb4R7Em_6RC"
      },
      "outputs": [],
      "source": [
        "gemma_qa = GemmaQA()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B98EFX7O_6RD"
      },
      "source": [
        "For start, we are testing the model with some of the data from the training set itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7BRI-TC_6RD"
      },
      "source": [
        "## Sample 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:51:45.448282Z",
          "iopub.status.busy": "2024-10-28T00:51:45.447832Z",
          "iopub.status.idle": "2024-10-28T00:51:45.778444Z",
          "shell.execute_reply": "2024-10-28T00:51:45.777544Z",
          "shell.execute_reply.started": "2024-10-28T00:51:45.448256Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "NZDGhLQc_6RD",
        "outputId": "5a227e46-d10f-4165-efe5-fad4989a3e4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            question  \\\n",
              "0  What actions can local governments take to pro...   \n",
              "1  Where can I access a calculator for determinin...   \n",
              "2  What effects does plastic waste have on carbon...   \n",
              "3  What does the abbreviation 'TNC' represent in ...   \n",
              "4  How can we reduce carbon footprints using sust...   \n",
              "\n",
              "                                              answer  \n",
              "0  Local governments can promote carbon footprint...  \n",
              "1  Yes, you can estimate your flight pollution us...  \n",
              "2  Plastic waste significantly contributes to **c...  \n",
              "3  TRANSNATIONAL CORPORATION:\\nA company that ope...  \n",
              "4  Renewable energy sources like **solar** and **...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36cafade-b139-4f1d-8c32-55b460289453\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What actions can local governments take to pro...</td>\n",
              "      <td>Local governments can promote carbon footprint...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Where can I access a calculator for determinin...</td>\n",
              "      <td>Yes, you can estimate your flight pollution us...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What effects does plastic waste have on carbon...</td>\n",
              "      <td>Plastic waste significantly contributes to **c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What does the abbreviation 'TNC' represent in ...</td>\n",
              "      <td>TRANSNATIONAL CORPORATION:\\nA company that ope...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>How can we reduce carbon footprints using sust...</td>\n",
              "      <td>Renewable energy sources like **solar** and **...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36cafade-b139-4f1d-8c32-55b460289453')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36cafade-b139-4f1d-8c32-55b460289453 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36cafade-b139-4f1d-8c32-55b460289453');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-72be8703-4b8f-473b-8a91-da626f1021ad\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-72be8703-4b8f-473b-8a91-da626f1021ad')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-72be8703-4b8f-473b-8a91-da626f1021ad button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 45863,\n  \"fields\": [\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3295,\n        \"samples\": [\n          \"What are some notable sources of regenerative energy?\",\n          \"Why is using mass transit often more sustainable than private vehicle usage?\",\n          \"How are **heat-trapping gases** measured, estimated, and reported?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4032,\n        \"samples\": [\n          \"Improving **energy efficiency** in appliances can lead to substantial reductions in electricity use, significantly lowering carbon gas emissions \\nat both individual and collective levels.\",\n          \"You should refer to reliable and quality-assured sources of environmental change science,\\nsuch as **peer-reviewed papers**.\",\n          \"Carbon footprint audits are essential for organizations to identify significant sources of toxic emissions, \\nallowing them to create targeted strategies for reduction.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "df =  pd.read_csv(\"/content/drive/MyDrive/Hungers_data/format1.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:51:45.780186Z",
          "iopub.status.busy": "2024-10-28T00:51:45.779810Z",
          "iopub.status.idle": "2024-10-28T00:51:45.787120Z",
          "shell.execute_reply": "2024-10-28T00:51:45.786216Z",
          "shell.execute_reply.started": "2024-10-28T00:51:45.780154Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "vM4Ps0ZC_6RM",
        "outputId": "2a0f047e-3ff8-4e0a-97f5-0be0197e8341"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "question    What does production of **meat and dairy produ...\n",
              "answer      Livestock production contributes to climate ch...\n",
              "Name: 15, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>15</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>What does production of **meat and dairy produ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <td>Livestock production contributes to climate ch...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "df.iloc[15]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:51:45.790818Z",
          "iopub.status.busy": "2024-10-28T00:51:45.790537Z",
          "iopub.status.idle": "2024-10-28T00:52:08.880613Z",
          "shell.execute_reply": "2024-10-28T00:52:08.879680Z",
          "shell.execute_reply.started": "2024-10-28T00:51:45.790795Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "MIxQ65cq_6RN",
        "outputId": "c99c8cb5-11a9-44e0-a76d-1e95f6a1e506"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat does production of **meat and dairy products** have to do with climate disruption?\n\n**<font color='green'>Answer:</font>**\nLivestock production contributes to climate change through:\n- **Methane discharge**\n- **Deforestation** for animal agriculture,\naccounting for a significant percentage of **global greenhouse gas discharge**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "row = df.iloc[15]\n",
        "gemma_qa.query(row.loc[\"question\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:08.882368Z",
          "iopub.status.busy": "2024-10-28T00:52:08.882060Z",
          "iopub.status.idle": "2024-10-28T00:52:31.257467Z",
          "shell.execute_reply": "2024-10-28T00:52:31.256487Z",
          "shell.execute_reply.started": "2024-10-28T00:52:08.882343Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtMs-8uT_6RN",
        "outputId": "29c0aa42-8bb8-42f6-b049-c9d4324dbe51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "Other than carbon foot print name other foot print?\n",
            "\n",
            "Answer:\n",
            "**Ecological footprint** means the total carbon airborne pollutants from a person, event or product's lifecycle. Apart from carbon carbon releases, **ecological footprint** also includes:\n",
            "- **Energy consumption**\n",
            "- **Waste produced**\n",
            "- **Water usage**\n",
            "- **Reuse and recycling**.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    question=\"Other than carbon foot print name other foot print?\",\n",
        "    answer=\"\",\n",
        ")\n",
        "print(gemma_causal_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:31.259449Z",
          "iopub.status.busy": "2024-10-28T00:52:31.258999Z",
          "iopub.status.idle": "2024-10-28T00:52:32.347737Z",
          "shell.execute_reply": "2024-10-28T00:52:32.346774Z",
          "shell.execute_reply.started": "2024-10-28T00:52:31.259414Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYeXVQxF_6RO",
        "outputId": "07e5c15a-97f1-4344-d2fa-09aae0f03cf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "Who are you?\n",
            "\n",
            "Answer:\n",
            "I'm a researcher at the University of Bath in England,\n",
            "specialising in energy optimisation and carbon toxic emissions of buildings and supply chains.\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    question=\"Who are you?\",\n",
        "    answer=\"\",\n",
        ")\n",
        "print(gemma_causal_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:32.349577Z",
          "iopub.status.busy": "2024-10-28T00:52:32.349267Z",
          "iopub.status.idle": "2024-10-28T00:52:33.814310Z",
          "shell.execute_reply": "2024-10-28T00:52:33.813366Z",
          "shell.execute_reply.started": "2024-10-28T00:52:32.349549Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2yYOE-d_6RO",
        "outputId": "5ab7e736-6de6-4a64-a862-1de402c8cacb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:\n",
            "List two types of caron foot prints?\n",
            "\n",
            "Answer:\n",
            "Two\n",
            "of the\n",
            "most important **types** of\n",
            "carbon releases are **direct release** (from sources like fuel combustion) and **indirect release** (based on resource extraction and transportation).\n"
          ]
        }
      ],
      "source": [
        "prompt = template.format(\n",
        "    question=\"List two types of caron foot prints?\",\n",
        "    answer=\"\",\n",
        ")\n",
        "print(gemma_causal_lm.generate(prompt, max_length=256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y9XcLL4_6RO"
      },
      "source": [
        "## Sample 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:33.817300Z",
          "iopub.status.busy": "2024-10-28T00:52:33.815534Z",
          "iopub.status.idle": "2024-10-28T00:52:37.474231Z",
          "shell.execute_reply": "2024-10-28T00:52:37.473263Z",
          "shell.execute_reply.started": "2024-10-28T00:52:33.817270Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "E9l2RMgm_6RP",
        "outputId": "af8794cd-d15b-4344-8cfd-15d9680f2189"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat are the research topics related to climate footprint research in electrical and computer engineering?\n\n**<font color='green'>Answer:</font>**\nThere are several **research topics** related to carbon footprint in electrical and computer engineering, including:\n- **Predictive control** of energy management in electricity generation from **solar thermal energy**.\n- **Optimization** of the energy efficiency of high-consuming electrical systems such as **heating** and **cooling**.\n- **Management** of loading and unloading of **thermal solar energy storage systems** for electric generation plants."
          },
          "metadata": {}
        }
      ],
      "source": [
        "row = df.iloc[8]\n",
        "gemma_qa.query(row.question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcVv5elL_6RP"
      },
      "source": [
        "## Sample 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:37.476018Z",
          "iopub.status.busy": "2024-10-28T00:52:37.475669Z",
          "iopub.status.idle": "2024-10-28T00:52:39.558912Z",
          "shell.execute_reply": "2024-10-28T00:52:39.558022Z",
          "shell.execute_reply.started": "2024-10-28T00:52:37.475987Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Qxw9J8Re_6RP",
        "outputId": "d6b1067f-c4e2-4a3d-eba2-019480649acf"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhich energy resource has the smallest climate footprint?\n\n**<font color='green'>Answer:</font>**\n**Wind energy** has the smallest carbon footprint of all energy types, emitting **11 grams of CO2 equivalent** per kWh during production."
          },
          "metadata": {}
        }
      ],
      "source": [
        "row = df.iloc[25]\n",
        "gemma_qa.query(row.question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSruif-T_6RQ"
      },
      "source": [
        "## Not seen question(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:39.560350Z",
          "iopub.status.busy": "2024-10-28T00:52:39.560064Z",
          "iopub.status.idle": "2024-10-28T00:52:41.563088Z",
          "shell.execute_reply": "2024-10-28T00:52:41.562090Z",
          "shell.execute_reply.started": "2024-10-28T00:52:39.560325Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "fBAayx6e_6RQ",
        "outputId": "42e7ba20-9dfc-403e-e374-65f7b30486ae"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat is Climate Change?\n\n**<font color='green'>Answer:</font>**\nClimate change refers to the long-term alteration of temperature and typical weather patterns in a place,\nprimarily driven by human activities that increase climate pollutants in the atmosphere,\nsuch as burning fossil fuels and deforestation."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"What is Climate Change?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:41.564380Z",
          "iopub.status.busy": "2024-10-28T00:52:41.564116Z",
          "iopub.status.idle": "2024-10-28T00:52:44.038894Z",
          "shell.execute_reply": "2024-10-28T00:52:44.037926Z",
          "shell.execute_reply.started": "2024-10-28T00:52:41.564357Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "xr5184Pt_6RQ",
        "outputId": "4e40d259-3769-4b29-cfa4-14c24c0f06d3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nList ways to reduce carbon foot print?\n\n**<font color='green'>Answer:</font>**\nEffective strategies to reduce carbon emissions footprint include:\n- **Using renewable energy**\n- **Improving energy efficiency**\n- **Promoting regenerative energy**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"List ways to reduce carbon foot print?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:44.040578Z",
          "iopub.status.busy": "2024-10-28T00:52:44.040216Z",
          "iopub.status.idle": "2024-10-28T00:52:45.492706Z",
          "shell.execute_reply": "2024-10-28T00:52:45.491752Z",
          "shell.execute_reply.started": "2024-10-28T00:52:44.040544Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "NLCRZAO-_6RR",
        "outputId": "f6da0bd5-ccb6-418b-f1d9-40abd25cf271"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat is a code competition?\n\n**<font color='green'>Answer:</font>**\nA **computer programming competition** is a contest in which participants solve **complex programming problems** using **different programming languages**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"What is a code competition?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:45.494134Z",
          "iopub.status.busy": "2024-10-28T00:52:45.493831Z",
          "iopub.status.idle": "2024-10-28T00:52:47.771201Z",
          "shell.execute_reply": "2024-10-28T00:52:47.770302Z",
          "shell.execute_reply.started": "2024-10-28T00:52:45.494107Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "tlciGP2Q_6RR",
        "outputId": "d32c8851-3f65-4e35-b4b9-2aeff275a6d7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat are the steps to create a Kaggle dataset?\n\n**<font color='green'>Answer:</font>**\nTo create **Kaggle datasets**, you can:\n- **Upload or create** files with machine learning algorithms and model outputs on the **platform**.\n- **Link** files to specific models for evaluation."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"What are the steps to create a Kaggle dataset?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:58:51.062524Z",
          "iopub.status.busy": "2024-10-28T00:58:51.062098Z",
          "iopub.status.idle": "2024-10-28T00:58:53.391249Z",
          "shell.execute_reply": "2024-10-28T00:58:53.390320Z",
          "shell.execute_reply.started": "2024-10-28T00:58:51.062486Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "GChpb3gD_6RR",
        "outputId": "bdb80cfe-30f3-49b5-ec96-cac45b10b095"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nHow can I reduce my carbon foot print as a Multimedia University of Kenya Student?\n\n**<font color='green'>Answer:</font>**\nAdopting **eco-friendly practices** can help reduce carbon emissions footprint, especially if the technology utilizes **renewable energy**."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"How can I reduce my carbon foot print as a Multimedia University of Kenya Student?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:59:30.071082Z",
          "iopub.status.busy": "2024-10-28T00:59:30.070724Z",
          "iopub.status.idle": "2024-10-28T00:59:31.141327Z",
          "shell.execute_reply": "2024-10-28T00:59:31.140343Z",
          "shell.execute_reply.started": "2024-10-28T00:59:30.071055Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "ak9nC9z4_6RS",
        "outputId": "255e0765-0477-47d9-f939-9eb1f91fb00d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nHow Old are you?\n\n**<font color='green'>Answer:</font>**\nI am in my mid-thirties, so I have been **actively investing** for over a decade."
          },
          "metadata": {}
        }
      ],
      "source": [
        "question = \"How Old are you?\"\n",
        "gemma_qa.query(question)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How much carbon does an indivudual produce daily?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "6M-LlsjNXuMU",
        "outputId": "998093e9-4684-41bb-dcfa-5f808a06dc15"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nHow much carbon does an indivudual produce daily?\n\n**<font color='green'>Answer:</font>**\nOn average, an individualQuestion about their carbon trace needs more information about their **activities** and **footprint calculator** to calculate their daily carbon releases."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is water foot print?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "GBGyB_W8YGRz",
        "outputId": "7f9a5ae8-abf4-41dd-c92e-916d60999480"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat is water foot print?\n\n**<font color='green'>Answer:</font>**\nWater footprint measures the total volume of **freshwater outflow** (including co2 outflow from freshwater use),\nwhich helps to protect freshwater sources and mitigate **climate change**."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Can water foot print lead to carbon foot print?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "JRghv4TrYuV2",
        "outputId": "f2e63a91-be3e-4d67-bd02-861472458e67"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nCan water foot print lead to carbon foot print?\n\n**<font color='green'>Answer:</font>**\nYes, **carbon emissions footprint** can be reduced through improvements in **water footprint** pamamagitan ng:\n- **Water reuse**\n- **Efficient use of cold water**\n- **Installation of energy-efficient lighting**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"what should I do when I wake up?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "xbBmkrcUY5aM",
        "outputId": "c221d115-2549-44a8-9972-44884bf4bdc9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nwhat should I do when I wake up?\n\n**<font color='green'>Answer:</font>**\nBefore starting your day, consider mindful activities like yoga or meditation,\nwhich can improve your emotional and physical health."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display(df.iloc[30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "YLZ3VcXheO4X",
        "outputId": "7d6d8188-ef8b-4628-9f68-035eeec6156f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "question    In what ways do food preferences affect carbon...\n",
              "answer      Dietary choices impact **carbon emissions** si...\n",
              "Name: 30, dtype: object"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>30</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>question</th>\n",
              "      <td>In what ways do food preferences affect carbon...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>answer</th>\n",
              "      <td>Dietary choices impact **carbon emissions** si...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row = df.iloc[30]\n",
        "gemma_qa.query(row.question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "-5R8EA0hcIoF",
        "outputId": "3fa87596-b5d6-4203-de49-8b9461c6e812"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nIn what ways do food preferences affect carbon emissions?\n\n**<font color='green'>Answer:</font>**\nDietary choices impact **carbon emissions** significantly; consuming less meat and dairy and opting for a **plant-rich diet** can substantially lower your personal carbon mark."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is water?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "D359noale5rW",
        "outputId": "601069ac-3b7c-4a08-c2d7-cf2695b43cb4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat is water?\n\n**<font color='green'>Answer:</font>**\nWater conservation means reducing water usage while maintaining a **high quality** of life."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"What is Carbon pollution?\"\n",
        "gemma_qa.query(question)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "oZs1kDkIBjqT",
        "outputId": "23434599-44f9-468f-af72-4506684423f2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Question:\nWhat is Carbon pollution?\n\n**<font color='green'>Answer:</font>**\nCarbon emissions refer to the release of carbon into the atmosphere,\nprimarily from greenhouse gas emissions that contribute to climate change.\nThey are often measured as carbon dioxide equivalents."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDca_gCU_6RS"
      },
      "source": [
        "# Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:52:47.772712Z",
          "iopub.status.busy": "2024-10-28T00:52:47.772416Z",
          "iopub.status.idle": "2024-10-28T00:53:23.227706Z",
          "shell.execute_reply": "2024-10-28T00:53:23.226867Z",
          "shell.execute_reply.started": "2024-10-28T00:52:47.772686Z"
        },
        "id": "DKPF0hyA_6RT"
      },
      "outputs": [],
      "source": [
        "preset_dir = \".\\gemma2_2b_en_MNLR5_model_docs\"\n",
        "gemma_causal_lm.save_to_preset(preset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShoNb5-P_6RT"
      },
      "source": [
        "# Publish Model on Kaggle as a Kaggle Model\n",
        "\n",
        "We are publishing now the saved model as a Kaggle Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-28T00:53:23.229084Z",
          "iopub.status.busy": "2024-10-28T00:53:23.228802Z",
          "iopub.status.idle": "2024-10-28T00:56:36.360074Z",
          "shell.execute_reply": "2024-10-28T00:56:36.359114Z",
          "shell.execute_reply.started": "2024-10-28T00:53:23.229058Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev5hYdXs_6RU",
        "outputId": "c13ec000-93c8-4c41-87d6-2ec2543bdb7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uploading Model https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR5_model_docs ...\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/task.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading: 100%|██████████| 2.98k/2.98k [00:00<00:00, 7.65kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/task.json (3KB)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/tokenizer.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 591/591 [00:00<00:00, 1.37kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/tokenizer.json (591B)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/preprocessor.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 1.41k/1.41k [00:00<00:00, 3.97kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/preprocessor.json (1KB)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/metadata.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 143/143 [00:00<00:00, 361B/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/metadata.json (143B)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/model.weights.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 10.5G/10.5G [02:05<00:00, 83.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/model.weights.h5 (10GB)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 782/782 [00:00<00:00, 2.14kB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/config.json (782B)\n",
            "Starting upload for file .\\gemma2_2b_en_MNLR5_model_docs/assets/tokenizer/vocabulary.spm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Uploading: 100%|██████████| 4.24M/4.24M [00:00<00:00, 10.0MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Upload successful: .\\gemma2_2b_en_MNLR5_model_docs/assets/tokenizer/vocabulary.spm (4MB)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model instance has been created.\n",
            "Files are being processed...\n",
            "See at: https://www.kaggle.com/models/moanlobago/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR5_model_docs\n"
          ]
        }
      ],
      "source": [
        "kaggle_username = os.environ[\"KAGGLE_USERNAME\"]\n",
        "\n",
        "kaggle_uri = f\"kaggle://{kaggle_username}/gemma2-kaggle-docs/keras/gemma2_2b_en_MNLR5_model_docs\"\n",
        "keras_nlp.upload_preset(kaggle_uri, preset_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_Qtb6XB_6RU"
      },
      "source": [
        "# Conclusions\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66mrtjBk_6RU"
      },
      "source": [
        "We demonstated how to fine-tune a **Gemma 2** model using LoRA.   \n",
        "We also created a class to run queries to the **Gemma 2** model and tested it with some examples from the existing training data but also with some new, not seen questions.   \n",
        "We also saved the models as a Keras model.\n",
        "Then we published the model as a Kaggle Model on Kaggle Models platform."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tCbrealuBX8I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 4484051,
          "sourceId": 7711309,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5853991,
          "sourceId": 9596582,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5958248,
          "sourceId": 9735365,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5958454,
          "sourceId": 9735634,
          "sourceType": "datasetVersion"
        },
        {
          "modelId": 78150,
          "modelInstanceId": 72244,
          "sourceId": 85984,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30674,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}