{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Generative AI and CSV Processing\n",
    "\n",
    "## Installation\n",
    "\n",
    "First, ensure that you have the required package installed. You can do this by running:\n",
    "\n",
    "```python\n",
    "%pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T20:27:40.671521Z",
     "iopub.status.busy": "2024-11-14T20:27:40.671053Z",
     "iopub.status.idle": "2024-11-14T20:28:11.014062Z",
     "shell.execute_reply": "2024-11-14T20:28:11.012693Z",
     "shell.execute_reply.started": "2024-11-14T20:27:40.671462Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T20:28:11.016767Z",
     "iopub.status.busy": "2024-11-14T20:28:11.016358Z",
     "iopub.status.idle": "2024-11-14T20:28:12.230058Z",
     "shell.execute_reply": "2024-11-14T20:28:12.228917Z",
     "shell.execute_reply.started": "2024-11-14T20:28:11.016713Z"
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "Set your Google API key for the generative AI model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T20:28:12.232557Z",
     "iopub.status.busy": "2024-11-14T20:28:12.232042Z",
     "iopub.status.idle": "2024-11-14T20:28:12.451145Z",
     "shell.execute_reply": "2024-11-14T20:28:12.450170Z",
     "shell.execute_reply.started": "2024-11-14T20:28:12.232515Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Descriptions\n",
    "\n",
    "- **`Final_evaluated_1.csv`**\n",
    "  - **Description**: This file contains the results for **Model Version 1** after human evaluation.\n",
    "  - **Content**: The output includes performance metrics and evaluation results obtained after human review.\n",
    "\n",
    "- **`Final_evaluated_2.csv`**\n",
    "  - **Description**: This file contains the results for **Model Version 2** after human evaluation.\n",
    "  - **Content**: Similar to the first file, it includes performance metrics and evaluation results after being assessed by human evaluators.\n",
    "\n",
    "- **`Final_evaluated_3.csv`**\n",
    "  - **Description**: This file contains the results for **Model Version 3** after human evaluation.\n",
    "  - **Content**: This file also includes performance metrics and evaluation results from human review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T20:28:12.453608Z",
     "iopub.status.busy": "2024-11-14T20:28:12.453216Z",
     "iopub.status.idle": "2024-11-14T20:31:33.547964Z",
     "shell.execute_reply": "2024-11-14T20:31:33.546572Z",
     "shell.execute_reply.started": "2024-11-14T20:28:12.453566Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Gemini Score for /kaggle/input/model-1data/model_responses_with_time_tqdm1S .csv: 45.9\n",
      "Total Gemini Score for /kaggle/input/model-2data/model_responses_with_time_tqdm2S.csv: 48.58\n",
      "Total Gemini Score for /kaggle/input/model-3data/model_responses_with_time_tqdm3S.csv: 47.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "\n",
    "# Parameters\n",
    "file_paths = [\n",
    "    '/kaggle/input/model-1data/model_responses_with_time_tqdm1S .csv',\n",
    "    '/kaggle/input/model-2data/model_responses_with_time_tqdm2S.csv',\n",
    "    '/kaggle/input/model-3data/model_responses_with_time_tqdm3S.csv'\n",
    "]\n",
    "\n",
    "output_paths = [\n",
    "    './Final_evaluated_1.csv',\n",
    "    './Final_evaluated_2.csv',\n",
    "    './Final_evaluated_3.csv'\n",
    "]\n",
    "\n",
    "max_tokens = 200  # Maximum tokens for the model response\n",
    "\n",
    "# Initialize the Gemini model for evaluating answers.\n",
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=max_tokens)\n",
    ")\n",
    "\n",
    "def extract_and_evaluate(row):\n",
    "    \"\"\"\n",
    "    Evaluates the model's answer against the expected answer.\n",
    "\n",
    "    Parameters:\n",
    "    row (pd.Series): A row of the DataFrame containing 'question', 'expected_answer', 'model_answer', and 'Max_Score'.\n",
    "\n",
    "    Returns:\n",
    "    tuple: Extracted model answer, score, and reasoning.\n",
    "    \"\"\"\n",
    "    question = row['question']\n",
    "    expected_answer = row['expected_answer']\n",
    "    model_answer = row['model_answer']\n",
    "    max_score = row['Max_Score']\n",
    "\n",
    "    # Extract the part of the model answer that is actually the answer\n",
    "    answer_start = model_answer.find(\"Answer:\") + len(\"Answer:\")\n",
    "    model_answer_extracted = model_answer[answer_start:].strip() if answer_start > -1 else \"No valid answer found.\"\n",
    "\n",
    "    # Prepare the evaluation prompt\n",
    "    prompt = (\n",
    "        f\"Given the question below, evaluate the provided answer and score it from 0 to {max_score} based on its alignment with the expected answer.\\n\"\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Expected Answer: {expected_answer}\\n\"\n",
    "        f\"Model Answer: {model_answer_extracted}\\n\"\n",
    "        \"Please provide a score and brief reasoning, focusing on conceptual alignment.\"\n",
    "    )\n",
    "\n",
    "    # Use the model to evaluate the extracted answer\n",
    "    try:\n",
    "        evaluation_response = short_model.generate_content(prompt).text.strip()\n",
    "        \n",
    "        # Extract score and reasoning using regex\n",
    "        score_match = re.search(r'Score:\\s*[\\*]*(\\d*\\.?\\d+)[\\*]*', evaluation_response)\n",
    "        score = float(score_match.group(1)) if score_match else 0\n",
    "        \n",
    "        reasoning_match = re.search(r'Reasoning:\\s*([\\s\\S]*)', evaluation_response)\n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else \"Reasoning not provided.\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing evaluation response for question '{question}': {e}\")\n",
    "        score = 0\n",
    "        reasoning = \"Error in evaluation.\"\n",
    "\n",
    "    return model_answer_extracted, score, reasoning\n",
    "\n",
    "# Process each file\n",
    "for i, (data_path, output_path) in enumerate(zip(file_paths, output_paths)):\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(data_path)  # Make sure this path is correct\n",
    "\n",
    "    # Apply the function to all rows in the DataFrame\n",
    "    results = df.apply(extract_and_evaluate, axis=1)\n",
    "\n",
    "    # Expand the results into separate columns\n",
    "    df['Extracted Model Answer'] = results.str[0]\n",
    "    df['Gemini Score'] = results.str[1]\n",
    "    df['Evaluation Reasoning'] = results.str[2]\n",
    "\n",
    "    # Save the updated DataFrame back to CSV\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "    # Print the sum of the Gemini Score for the current file\n",
    "    total_score = df['Gemini Score'].sum()\n",
    "    print(f\"Total Gemini Score for {data_path}: {total_score}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6087721,
     "sourceId": 9908388,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6087841,
     "sourceId": 9908540,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6087857,
     "sourceId": 9908573,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
